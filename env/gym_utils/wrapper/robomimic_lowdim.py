"""
Environment wrapper for Robomimic environments with state observations.

Also return done=False since we do not terminate episode early.

Modified from https://github.com/real-stanford/diffusion_policy/blob/main/diffusion_policy/env/robomimic/robomimic_lowdim_wrapper.py

For consistency, we will use Dict{} for the observation space, with the key "state" for the state observation.
"""

import numpy as np
import gym
from gym import spaces
import imageio


class RobomimicLowdimWrapper(gym.Env):
	def __init__(
		self,
		env,
		normalization_path=None,
		low_dim_keys=[
			"robot0_eef_pos",
			"robot0_eef_quat",
			"robot0_gripper_qpos",
			"object",
		],
		clamp_obs=False,
		init_state=None,
		render_hw=(256, 256),
		render_camera_name="agentview",
		impedance_mode="fixed",
		controller_configs=None,
	):
		self.env = env
		self.init_state = init_state
		self.render_hw = render_hw
		self.render_camera_name = render_camera_name
		self.video_writer = None
		self.clamp_obs = clamp_obs

		# Setting up controller config parameters.
		self.impedance_mode = impedance_mode
		self.controller_configs = controller_configs
		self.default_damping = self.controller_configs["damping"]
		self.default_stiffness = self.controller_configs["kp"]
		self.damping_exp_scale = self.controller_configs["damping_limits"][1] / self.default_damping
		self.stiffness_exp_scale = self.controller_configs["kp_limits"][1] / self.default_stiffness

		# set up normalization
		self.normalize = normalization_path is not None
		if self.normalize:
			normalization = np.load(normalization_path)
			self.obs_min = normalization["obs_min"]
			self.obs_max = normalization["obs_max"]
			self.action_min = normalization["action_min"]
			self.action_max = normalization["action_max"]

		# setup spaces
		low = np.full(env.action_dimension, fill_value=-1)
		high = np.full(env.action_dimension, fill_value=1)
		self.action_space = gym.spaces.Box(
			low=low,
			high=high,
			shape=low.shape,
			dtype=low.dtype,
		)
		self.obs_keys = low_dim_keys
		self.observation_space = spaces.Dict()
		obs_example_full = self.env.get_observation()
		obs_example = np.concatenate(
			[obs_example_full[key] for key in self.obs_keys], axis=0
		)
		low = np.full_like(obs_example, fill_value=-1)
		high = np.full_like(obs_example, fill_value=1)
		self.observation_space["state"] = spaces.Box(
			low=low,
			high=high,
			shape=low.shape,
			dtype=np.float32,
		)
		self.step_count = 0

	def normalize_obs(self, obs):
		obs = 2 * (
			(obs - self.obs_min) / (self.obs_max - self.obs_min + 1e-6) - 0.5
		)  # -> [-1, 1]
		if self.clamp_obs:
			obs = np.clip(obs, -1, 1)
		return obs

	def unnormalize_action(self, action):
		action = (action + 1) / 2  # [-1, 1] -> [0, 1]
		return action * (self.action_max - self.action_min) + self.action_min

	def get_observation(self, raw_obs):
		obs = {"state": np.concatenate([raw_obs[key] for key in self.obs_keys], axis=0)}
		if self.normalize:
			obs["state"] = self.normalize_obs(obs["state"])
		return obs

	def seed(self, seed=None):
		if seed is not None:
			np.random.seed(seed=seed)
		else:
			np.random.seed()

	def reset(self, options={}, **kwargs):
		"""Ignore passed-in arguments like seed"""
		# Close video if exists
		if self.video_writer is not None:
			self.video_writer.close()
			self.video_writer = None

		# Start video if specified
		if "video_path" in options:
			self.video_writer = imageio.get_writer(options["video_path"], fps=30)

		# Call reset
		new_seed = options.get(
			"seed", None
		)  # used to set all environments to specified seeds
		if self.init_state is not None:
			# always reset to the same state to be compatible with gym
			# raw_obs = self.env.reset_to({"states": self.init_state})
			raw_obs = self.env.reset_to(self.init_state)
		elif new_seed is not None:
			self.seed(seed=new_seed)
			raw_obs = self.env.reset()
		else:
			# random reset
			raw_obs = self.env.reset()
		self.step_count = 0
		return self.get_observation(raw_obs)

	def step(self, action):
		# Parse action based on impedance mode - updating control parameters is handled later.
		if self.impedance_mode == "variable":
			damping, kp, delta = action[:6], action[6:12], action[12:]
			# Un-normalizing/scaling damping and kp actions.
			damping = np.power(self.damping_exp_scale, damping) * self.default_damping
			kp = np.power(self.stiffness_exp_scale, kp) * self.default_stiffness
		elif self.impedance_mode == "variable_kp":
			kp, delta = action[:6], action[6:]
			# Un-normalizing/scaling kp action.
			kp = np.power(self.stiffness_exp_scale, kp) * self.default_stiffness
		else:
			delta = action

		# Un-normalizing operational space action (delta), if necessary.
		if self.normalize:
			delta = self.unnormalize_action(delta)

		# Re-combining full action based on impedance mode.
		if self.impedance_mode == "variable":
			raw_action = np.concatenate([damping, kp, delta], axis=0)
		elif self.impedance_mode == "variable_kp":
			raw_action = np.concatenate([kp, delta], axis=0)
		else:
			raw_action = delta
		
		# Stepping environment.
		raw_obs, reward, done, info = self.env.step(raw_action)
		obs = self.get_observation(raw_obs)

		# Adding delta, damping, and stiffness to info dict.
		info["delta"] = delta
		if self.impedance_mode == "variable":
			info["damping"] = damping
			info["stiffness"] = kp
		elif self.impedance_mode == "variable_kp":
			info["damping"] = np.array([self.default_damping] * 6)
			info["stiffness"] = kp
		else:
			info["damping"] = np.array([self.default_damping] * 6)
			info["stiffness"] = np.array([self.default_stiffness] * 6)
		# TODO: handle impedance modes properly

		# render if specified
		if self.video_writer is not None:
			video_img = self.render(mode="rgb_array")
			self.video_writer.append_data(video_img)
		self.step_count += 1
		return obs, reward, False, info

	def render(self, mode="rgb_array"):
		h, w = self.render_hw
		return self.env.render(
			mode=mode,
			height=h,
			width=w,
			camera_name=self.render_camera_name,
		)
